{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn.tree import _tree\n",
    "import pydotplus\n",
    "from IPython.display import Image, display, clear_output\n",
    "from ipywidgets import widgets, Layout\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from bokeh.plotting import *\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from tabulate import tabulate\n",
    "import squarify\n",
    "import spacy\n",
    "import string\n",
    "from nltk.classify import SklearnClassifier\n",
    "from subprocess import check_output\n",
    "#from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import string\n",
    "import base64\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import holoviews as hv\n",
    "import copy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "from time import time\n",
    "stopwords = stopwords.words('english')\n",
    "sns.set_context('talk')\n",
    "\n",
    "import re\n",
    "#spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import tabulate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from spacy import displacy\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2523, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes= [\"SW_EpisodeIV.txt\",\"SW_EpisodeV.txt\", \"SW_EpisodeVI.txt\"]\n",
    "SW= []\n",
    "star_all = pd.DataFrame()\n",
    "for episode in episodes:\n",
    "    star_1 = pd.read_csv(episode, sep='\" \"', engine='python')\n",
    "    star_1 = star_1.loc[:,star_1.dtypes==object].apply(lambda star_1:star_1.str.replace('\"', \"\"))\n",
    "    star_1 = star_1.reset_index(drop=True)\n",
    "    star_1.columns = ['character', 'dialogue']\n",
    "    SW.append(star_1)\n",
    "star_all = pd.concat(SW)\n",
    "star_all = star_all.reset_index(drop=True)\n",
    "star_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "classes=star_all.character.value_counts()\n",
    "classes_big=classes[classes>=40]\n",
    "sufficient=star_all[star_all.character.isin(classes_big.index)][[\"character\",'dialogue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1930, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sufficient=sufficient.reset_index(drop=True)\n",
    "sufficient.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sufficient[\"tokens\"]=sufficient['dialogue'].apply(word_tokenize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def no_punct(tokens):\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    return words\n",
    "def to_lower(tokens):\n",
    "    words = [w.lower() for w in tokens]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sufficient[\"tokens\"]=sufficient['tokens'].apply(to_lower)\n",
    "sufficient[\"tokens\"]=sufficient['tokens'].apply(no_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def cleanup_text(docs, logging=False):\n",
    "    texts = []\n",
    "    counter = 1\n",
    "    for doc in docs:\n",
    "        if counter % 1000 == 0 and logging:\n",
    "            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n",
    "        counter += 1\n",
    "        doc = nlp(doc, disable=['parser', 'ner'])\n",
    "        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
    "        tokens = ' '.join(tokens)\n",
    "        texts.append(tokens)\n",
    "    return pd.Series(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66365343 1.5335983  0.8064194  2.6753042 ]\n",
      "[0.745111  1.931929  0.7636134 2.2745903]\n",
      "0.9531291379577567\n"
     ]
    }
   ],
   "source": [
    "#the order of words is been taken into account\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "a = nlp('the test sentence')\n",
    "b = nlp('the sentence test')\n",
    "print(a.vector[:4])\n",
    "print(b.vector[:4])\n",
    "print(a.similarity(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sufficient[\"words\"]=sufficient.tokens.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def cleanup_text(docs, logging=False):\n",
    "    texts = []\n",
    "    counter = 1\n",
    "    for doc in docs:\n",
    "        if counter % 1000 == 0 and logging:\n",
    "            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n",
    "        counter += 1\n",
    "        doc = nlp(doc, disable=['parser', 'ner'])\n",
    "        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
    "        tokens = ' '.join(tokens)\n",
    "        texts.append(tokens)\n",
    "    return pd.Series(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    #changed to \"argmax\" of the scores\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 1e-8  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 1e-8  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i][\"cats\"]\n",
    "        \n",
    "        #here: set label and score to the highest of the predictions. Compare to truth\n",
    "        #Recall basically deactivated\n",
    "        label, score = sorted(doc.cats.items(), key=lambda val: val[1], reverse=True)[0]\n",
    "        #for label, score in doc.cats.items():\n",
    "        if gold[label] >= 0.5:\n",
    "            tp += 1.\n",
    "        elif gold[label] < 0.5:\n",
    "            fp += 1.\n",
    "        #elif score < 0.5 and gold[label] < 0.5:\n",
    "        #    tn += 1\n",
    "        #elif score < 0.5 and gold[label] >= 0.5:\n",
    "        #    fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy with blank model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Training the model...\n",
      "LOSS \t  P  \n",
      "426.509\t0.251\n",
      "319.878\t0.316\n",
      "269.537\t0.352\n",
      "240.000\t0.404\n",
      "210.097\t0.433\n",
      "185.652\t0.430\n",
      "170.108\t0.427\n",
      "151.407\t0.435\n",
      "137.181\t0.438\n",
      "127.999\t0.409\n",
      "I am your father [('LUKE', 0.7716049551963806), ('VADER', 0.05292923375964165), ('LEIA', 0.05280479043722153), ('BEN', 0.03298528492450714), ('LANDO', 0.02779574505984783), ('THREEPIO', 0.02031521685421467), ('HAN', 0.015233706682920456), ('EMPEROR', 0.011022736318409443), ('YODA', 0.0011719156755134463)]\n",
      "Saved model to .\n"
     ]
    }
   ],
   "source": [
    "def clean_string(mystring):\n",
    "    return re.sub('[^A-Za-z\\ 0-9 ]+', '', mystring)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model=None\n",
    "output_dir=\"\"\n",
    "n_iter=10\n",
    "if model is not None:\n",
    "    nlp = spacy.load(model)  # load existing spaCy model\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    print(\"Created blank 'en' model\")\n",
    "\n",
    "# add the text classifier to the pipeline if it doesn't exist\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "if 'textcat' not in nlp.pipe_names:\n",
    "    textcat = nlp.create_pipe('textcat')\n",
    "    nlp.add_pipe(textcat, last=True)\n",
    "# otherwise, get it, so we can add labels to it\n",
    "else:\n",
    "    textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "# add label to text classifier\n",
    "for i in ['THREEPIO', 'VADER', 'LUKE', 'BEN', 'LEIA', 'HAN', 'EMPEROR','YODA', 'LANDO']: #hier standen emotionen\n",
    "    textcat.add_label(i)\n",
    "\n",
    "\n",
    "df = sufficient\n",
    "\n",
    "character_values = df['character'].unique()\n",
    "labels_default = dict((v, 0) for v in character_values)\n",
    "\n",
    "train_data = []\n",
    "dev_data = []\n",
    "dev_texts = []\n",
    "dev_cats = []\n",
    "data_all=[]\n",
    "for i, row in df.iterrows():\n",
    "\n",
    "    label_values = copy.deepcopy(labels_default)\n",
    "    label_values[row['character']] = 1\n",
    "\n",
    "    data_all.append(((clean_string(row['dialogue'])), {\"cats\": label_values}))\n",
    "\n",
    "\n",
    "train_data ,dev_data = train_test_split(data_all,test_size=0.2)     \n",
    "\n",
    "\n",
    "for i in range(len(dev_data)):\n",
    "    dev_texts.append(dev_data[i][0])#dialogue\n",
    "    dev_cats.append(dev_data[i][1])\n",
    "\n",
    "# get names of other pipes to disable them during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "nlp.vocab.vectors.name = 'spacy_pretrained_vectors'\n",
    "with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "    optimizer = nlp.begin_training()\n",
    "    print(\"Training the model...\")\n",
    "    print('{:^5}\\t{:^5}'.format('LOSS', 'P'))\n",
    "    for i in range(n_iter):\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                       losses=losses)\n",
    "        with textcat.model.use_params(optimizer.averages):\n",
    "            # evaluate on the dev data split off in load_data()\n",
    "            scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "        print('{0:.3f}\\t{1:.3f}'  # print a simple table\n",
    "              .format(losses['textcat'], scores['textcat_p']))\n",
    "\n",
    "\n",
    "# test the trained model\n",
    "test_text = \"I am your father\"\n",
    "doc = nlp(test_text)\n",
    "print(test_text, sorted(doc.cats.items(), key=lambda val: val[1], reverse=True))\n",
    "\n",
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# spaCy with pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'en_core_web_sm'\n",
      "Training the model...\n",
      "LOSS \t  P  \n",
      "421.440\t0.303\n",
      "304.903\t0.326\n",
      "262.564\t0.355\n",
      "225.038\t0.391\n",
      "197.946\t0.381\n",
      "174.963\t0.394\n",
      "159.480\t0.407\n",
      "144.166\t0.399\n",
      "131.665\t0.407\n",
      "115.822\t0.407\n",
      "I am your father [('LUKE', 0.9127466082572937), ('LEIA', 0.028731418773531914), ('VADER', 0.023627549409866333), ('BEN', 0.013822007924318314), ('EMPEROR', 0.008743206039071083), ('LANDO', 0.005537025630474091), ('THREEPIO', 0.0024341237731277943), ('HAN', 0.0021043228916823864), ('YODA', 0.0008121429709717631)]\n",
      "Saved model to .\n"
     ]
    }
   ],
   "source": [
    "def clean_string(mystring):\n",
    "    return re.sub('[^A-Za-z\\ 0-9 ]+', '', mystring)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model=\"en_core_web_sm\" #included word-vectors\n",
    "output_dir=\"\"\n",
    "n_iter=10\n",
    "if model is not None:\n",
    "    nlp = spacy.load(model)  # load existing spaCy model\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    print(\"Created blank 'en' model\")\n",
    "\n",
    "# add the text classifier to the pipeline if it doesn't exist\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "if 'textcat' not in nlp.pipe_names:\n",
    "    textcat = nlp.create_pipe('textcat')\n",
    "    nlp.add_pipe(textcat, last=True)\n",
    "# otherwise, get it, so we can add labels to it\n",
    "else:\n",
    "    textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "# add label to text classifier\n",
    "for i in ['THREEPIO', 'VADER', 'LUKE', 'BEN', 'LEIA', 'HAN', 'EMPEROR','YODA', 'LANDO']: #hier standen emotionen\n",
    "    textcat.add_label(i)\n",
    "\n",
    "\n",
    "df = sufficient\n",
    "\n",
    "\n",
    "character_values = df['character'].unique()\n",
    "labels_default = dict((v, 0) for v in character_values)\n",
    "\n",
    "train_data = []\n",
    "dev_data = []\n",
    "dev_texts = []\n",
    "dev_cats = []\n",
    "data_all=[]\n",
    "for i, row in df.iterrows():\n",
    "\n",
    "    label_values = copy.deepcopy(labels_default)\n",
    "    label_values[row['character']] = 1\n",
    "\n",
    "    data_all.append(((clean_string(row['dialogue'])), {\"cats\": label_values}))\n",
    "    \n",
    "train_data ,dev_data = train_test_split(data_all,test_size=0.2)     \n",
    "\n",
    "for i in range(len(dev_data)):\n",
    "    dev_texts.append(dev_data[i][0])#dialogue\n",
    "    dev_cats.append(dev_data[i][1])\n",
    "\n",
    "# get names of other pipes to disable them during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "nlp.vocab.vectors.name = 'spacy_pretrained_vectors'\n",
    "with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "    optimizer = nlp.begin_training()\n",
    "    print(\"Training the model...\")\n",
    "    print('{:^5}\\t{:^5}'.format('LOSS', 'P'))\n",
    "    for i in range(n_iter):\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                       losses=losses)\n",
    "        with textcat.model.use_params(optimizer.averages):\n",
    "            # evaluate on the dev data split off in load_data()\n",
    "            scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "        print('{0:.3f}\\t{1:.3f}'  # print a simple table\n",
    "              .format(losses['textcat'], scores['textcat_p']))\n",
    "\n",
    "\n",
    "# test the trained model\n",
    "test_text = \"I am your father\"\n",
    "doc = nlp(test_text)\n",
    "print(test_text, sorted(doc.cats.items(), key=lambda val: val[1], reverse=True))\n",
    "\n",
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
